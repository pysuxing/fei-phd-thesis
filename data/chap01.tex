\chapter{绪论}

高性能计算技术是国家战略、经济、科技、安全等方面的
重要体现和是提升综合国力的重要手段和工具。
在能源、环境、气候、医疗、生物等等方面都迫切需求高性能计算。
过去三十年， 计算能力呈指数增长，
高性能计算技术已经成为世界各国国力实力体现之一。
2008年，高性能计算机的运算能力达到每秒$10^{15}$浮点计算（flops）。
2017年11月TOP500的排名中，
运算能力最快的"神威$\cdot$ 太湖之光"高性能计算机已经
达到了$93.015\times10^{15}$flops。
预计在2020年，第一台百亿亿次（E级）高性能计算机将问世。
但是，研制E级高性能计算机仍然面临着巨大的挑战和问题
\upcite{廖湘科2016新型高性能计算系统与技术}。
无论是能耗、通信、编程还是存储和可靠性，
无论是计算模型还是大规模并行框架，随着系统规模的增加，
仅依靠使用已有的技术并不能使系统性能大幅度增长。
如，天河二号高性能计算机运算能力$33.86\times10^{15}$flops，能耗17.8MW。
若是采用已有的技术，E级高性能计算机的能耗保守估计约534MW，
这大大超出所能承受的范围。
同时，大数据、云计算等业务的需求持续增长，对传统高性能计算提出新的挑战和需求。
美国、日本、德国等国均对E级高性能计算系统的研制做出了相应的规划和部署，
Intel、Nvidia、IBM、Cray等企业已经开始关键技术的攻关。
因此，E级高性能计算机研制具有重大的战略意义和经济价值。

\section{研究背景和意义}
本节介绍论文的研究背景，首先介绍了当前主要高性能计算机系统的基本情况。
其次分析了高性能互连网络对于高性能计算机系统的重要性，
并论述了高性能互连网络拓扑结构需要考虑的因素。

\subsection{高性能计算机}

\subsubsection{神威$\cdot$太湖之光}
“神威$\cdot$ 太湖之光” \upcite{Taihulight}是2017年11月TOP500的榜首，
2018年6月TOP500的第二名\upcite{top500}，
其运算峰值超过每秒 $10^{17}$ flops。
相比其他异构高性能计算机，
“神威$\cdot$ 太湖之光”的计算节点主要依靠国产自主的申威26010（SW26010）
异构众核处理器构成。
一个CPU涵盖260个处理单元，一个SW26010峰值性能可以超过$3\times10^{12}$ flops。
每8个计算节点通过PCB 板相连接。
每256个计算节点组成一个超级节点。
每个机柜由4个超级节点组成一个机柜。
总共40个机柜40960个计算节点。
“神威$\cdot$ 太湖之光” 采用三层不同层次的互连网络。
顶层是中央交换网络，负责不同超级节点间的数据交换。
中间一层是超级节点网络，每个超级节点内部256个计算节点全互连。
最底层是资源共享网络，提供服务和连接共享资源给超级节点。
网络的二分带宽为每秒$7\times10^{12}$ 字节。
网络直径为7。能耗为15.37MW。

\subsubsection{天河二号}
天河系列高性能计算机曾经七次占据TOP500的榜首\upcite{top500}。
天河二号高性能计算机在2017年11月TOP500中排名第二，
2018年6月TOP500的第四名\upcite{top500}。
天河二号\upcite{Liao2015} 的计算节点由2 个
Intel Xeon E5-2600处理器和3个 Intel Xeon Phi 加速器组成，
其峰值性能达$3.432\times10^{12}$ flops。
每个计算机柜有4个计算框架。
每个计算框架涵盖一个交换板、一个控制板和32个计算节点封装在16 个计算板上。
系统规模最大可构造144个机柜，
规模可达18432个计算节点和I/O节点。
天河二号互连网络使用的是24个8-lane端口的高阶路由芯片，
聚合带宽每秒$5.376\times10^{12}$比特。
天河二号采用的拓扑是三层胖树结构，
第一层结构中每32个计算节点连接在一块交换板上，
每个交换板是一个计算框架，
每四个计算框架组成一个机柜。
每个交换板是由6个24端口的芯片连接而成的两层胖树，
共引出36条光纤连接第二层的交换芯片。
第二层的交换芯片是24 端口的，
分别给12个端口连接第三层和第一层，
连接第一层的12个端口分别用来连接不同的12个计算框架，
也就是3 个机柜。
第三层实际上是个两层胖树结构，
由6 个24 端口的芯片连接而成的48 端口交换机。

\subsubsection{Piz Daint}
Piz Daint是欧洲最快的高性能计算机，由瑞士联邦理工学院投资升级。
在2017年11 月TOP500中排名第三，2018年6月TOP500的第六名\upcite{top500}，
其运算速度达$19.590\times10^{15}$flops。
Piz Daint采用的是Cray XC50高性能计算机模型 \upcite{crayxc50}。
系统的互连网络采用的是Dragonfly结构\upcite{dragonfly}。
每四个计算节点组成一个计算板。
每16个计算板通过背板全互连成一个机架，每个机柜含有3个机架。
那么，每个机柜有192 个计算节点。
每两个机柜组成Dragonfly 网络中的group结构。
每个group内的6个机架通过电线全互连。
整个系统通过光缆全互连网络中的group。

\subsubsection{PRIMEHPC FX10}
PRIMEHPC FX10\upcite{primehpcfx10} 是继“K computer”后，
Fujitsu公司推出的新一代高性能计算机，
其峰值性能可达$23.2\times10^{15}$ flops。
系统采用的是低能耗SPARC64 IXfx处理器芯片，
整个系统规模最高可达1024个机柜，
含98304个计算节点和6144 个I/O节点，能耗约23MW。
互连网络如图\ref{tofuto}所示采用的是6D Mesh/Torus 结构，

以Tofu\upcite{tofu} 模块为单元进行XYZ三维连接，
Tofu 模块内嵌ABC三维结构。
网络中X维链路和Y维链路相连各个机柜，
Z维链路和B维链路相连各个系统板。
A维链路和C维链路相连4个节点构成一个系统板。
Fujitsu公司已经在研制新一代E级高性能计算机Post-FX10，
计划采用Tofu的升级版Tofu 2\upcite{tofu2} 进行互连，
该系统不仅加强CPU和网络的性能，
而且具备更高的封装性和低能耗来支持E级计算的规模。

\begin{figure}[htp]
\centering
\includegraphics[width=.8\textwidth,height=.4\textwidth]{tofu.png}
\caption{Tofu模块和6D Mesh/Torus结构\upcite{tofu}}
\label{tofuto}
\end{figure}

\subsection{高性能互连网络}

E级高性能计算机系统的研制和应用面临巨大的挑战，
对高性能互连网络来说则面临更大的挑战。
高性能互连网络是高性能计算机系统的重要组成部分，
其主要功能是完成计算节点I/O 节点之间的通信。
随着处理器性能按照摩尔定律提升、
计算系统规模的按照指数级增长，
互连网络越来越成为影响系统性能的主要因素\upcite{johnkim}。
高性能互连网络主要由四个重要部分构成：
拓扑结构，路由算法，拥塞控制以及物理器件。
拓扑结构决定了报文在网络中最短跳步数、
节点间可用的总路径数和二分带宽，
路由算法则决定了报文在网络中的实际跳步数。
物理器件主要指通过拓扑结构相连并封装在机柜内的路由器和物理链路。
实际高性能互连网络通信则是依靠路由算法和拥塞控制机制保证。
除此之外，制约一个高性能互连网络的设计还有
很多因素\upcite{duato2002interconnection}。
主要因素介绍如下:

\textbf{网络性能}消息延迟和网络的吞吐率是网络性能的主要指标。
消息延迟是指消息在源节点开始发送的时间与目的节点接收消息的时间之差。
消息延迟直接影响处理器和存储的使用。
吞吐率则是在单位时间内，网络所能传输最大的信息量。
因为成本开销等原因，网络容量不是一个无底洞，
一旦饱和网络不会再传输新注入的消息，
这严重限制了系统的性能。
这两个指标跟网络拓扑结构的网络直径和链路容量紧密相关。

\textbf{灵活性和扩展性}一个灵活的网络，在扩大或者缩小规模的同时，
相应的带宽需求也按需求按比例增减，以保证性能不降低。
否则，带宽将成为系统瓶颈，降低系统效率。
同时，在物理器件约束下，网络仍然能够支持良好的扩展性，并保证网络性能。

\textbf{应用负载}网络可以根据应用负载的不同将网络中的节点或者链路划分成多个子网，
使得应用负载之间互相不影响性能。
同时，这也是出于安全因素的考虑。

\textbf{物理封装和器件约束}结合实际物理封装因素考虑
如何保持高性能互连网络拓扑本身的优良特性是设计高性能互连网络拓扑的重要问题。
物理封装有两个主要限制影响拓扑设计，一是缆线的长度和数量。
随着光纤技术的发展，机柜内部因为距离较短采用电信号传输，
机柜之间则采用光纤保证长距离的通信质量。
随着系统规模的增大，不仅缆线的数量增加使得物理封装难度增加，
由于机柜数的增加，缆线长度也因此增加而造成网络延迟增加，
如光纤传输延迟为每米5 纳秒。
第二个限制因素则是芯片的带宽和引脚数。
芯片的带宽和引脚数受限于芯片的面积，
在高阶路由器和光纤技术等器件发展不成熟时期，
互连网络受器件约束只能采用节点度数小，
网络直径大的结构，如经典的$k-ary$ $n-cube$结构。
随着工艺的进步，高阶路由器等器件的技术已成熟，
大规模低直径高性能互连网络成为高性能互连网络的主要结构。
除了此之外，简单性和可维护性都跟拓扑设计紧密相关的。
一个好的高性能互连网络拓扑结构一定是易构造，易维护的。

\textbf{可靠性} 为了保证网络性能，高性能互连网络是无损网络，
不支持丢包和重传。
因此，高性能互连网络的可靠性是保证网络传输的重要指标。
一旦发生有错误发生，网络中有多条可选路径保证传输不受影响。
而且，某条链路或者节点发生错误，网络的连通度不受影响等，
都是设计网络结构的时候需要考虑的场景。

\textbf{成本能耗开销} 随着系统规模的增加，
路由器个数和缆线数量的增加，都大大增加了成本和能耗的开销。
尤其，在能耗方面，若是随着规模的增加而线性增长，
将无法完成E 级高性能计算机系统的开发。
因此，在设计网络拓扑结构时，成本能耗开销起着决定性的作用。

\subsection{研究现状和不足}

本节主要回顾国际上在高性能互连网络设计方面的相关工作。
自高性能计算机系统发展以来，在HPCA、ISCA、 SC、 ICS等高性能领域顶级会议上，
高性能互连网络一直都是热点研究方向。
Intel、Cray、IBM、Bull、Mellanox等公司以及
许多研究小组都在关注和致力于研究高性能互连网络，
针对高性能互连网络的拓扑结构设计，路由算法及拥塞控制等方面进行研究。

随着对系统规模的需求增大，以及高阶路由器结构和光纤技术的发展，
大规模高性能互连网络采用低直径拓扑结构成为高性能互连网络的发展趋势。
在高性能计算机系统发展早期，
大部分高性能计算机系统的互连网络采用的要么是树形或者是
$k-ary$ $n-fly$蝶形等间接网络结构，
要么是$k-ary$ $n-cube$直接网络结构。
这些结构的特点都是使用端口数较小的交换机通过多级或者笛卡尔积的方式构建网络。
2007年，Kim等人在ISCA顶级会议上提出了使用高阶路由器搭建大规模
高性能互连网络低直径拓扑结构Flattened Butterfly\upcite{Flattenedbutterfly}，
将高阶路由器替代传统的低阶路由器，从而减少路由器数量和降低网络直径。
之后，该组在2008年的ISCA会议上提出新型
高性能互连网络Dragonfly\upcite{dragonfly}结构和
2009年的超级计算机顶级会议SC上提出可灵活配置的HyperX结构\upcite{hyperx}，
不仅充分利用高阶路由器的特点还将光纤运用在拓扑结构的不同层次。
至此，使用高阶路由器成为构建高性能互连网络新型拓扑结构的主要手段。
Cray、IBM等大公司也开始使用高阶路由器搭建高性能计算系统，
如Cray公司的Cascade系统\upcite{cascade}和IBM的PERCS系统。

在高性能互连网络拓扑结构方面，
Koibuchi所在的研究小组在2012年的ISCA会议上提出了
使用高阶路由器搭建随机拓扑\upcite{acaserandom}，
其不仅网络直径低，平均最短路径小，还可以支持任意的网络规模。
但是，随机拓扑在物理布局中，面临缆线长短不一，连线复杂等不足，
2013年的HPCA会议上他们小组提出layout\-conscious的随机拓扑结构\upcite{fsorandom}，
根据实际物理降低缆线的长度减少系统构建的开销。
2015年的HPCA 会议上，他们小组提出在机柜上摆放自由光设备，
根据不同应用的负载，在随机拓扑上添加自由光链路以满足不同的链路需求。
2014年的SC会议上瑞士联邦理工的Besta和Hoefler利用代数图论的方法
构造出近似最优的大规模低直径拓扑Slim Fly\upcite{slimfly}。
Slim Fly结构在不受路由器端口数限制的条件下，相比其他结构，
不仅端口利用率高，而且获得较优的网络性能。
2015年SC会议上Kathareios等人提出一个性价比更高，
直径为2的拓扑结构OFT\upcite{costeffective2}。
在同样的成本开销下，OFT结构相比Slim Fly结构，拥有更好的可扩展性。

高性能互连网络的路由算法是高性能互连网络的研究重点。
一个拓扑即使具备再好的优良特性，
如果没有合适的路由算法也不能展现出优良的网络特性。
路由算法决定了拓扑结构的实际网络性能。
随着系统规模的增加，大规模低直径互连网络成为高性能互连网络的首选。
同时，给设计合适的路由算法带来巨大挑战。
Jiang等人在2009年ISCA会议上对Dragonfly结构提出
间接自适应路由通过本地链路间接反馈来解决全局链路的拥塞\upcite{indirect}。
随后，Garc$\'{\i}$a等人分析出现有的自适应路由算法在解决
Dragonfly结构全局链路拥塞问题的同时引入了本地链路的拥塞，
因此，提出On\-the\-fly路由算法解决全局和本地链路拥塞\upcite{On-the-Fly}的问题。
On\-the\-fly路由算法由于是自适应绕路路由，
如果采用虚通道隔离的方式避免死锁，
路由芯片需要增加虚通道数目而增加缓存资源和控制单元。
不仅会增加了成本开销而且受芯片面积限制。
如果采用逃逸子网的方式避免死锁，
逃逸路径较长会严重影响网络性能。
因此，Garc$\'{\i}$a 等人提出了OFAR-CM\upcite{OFAR-CM}、
RLM\upcite{Rlmolm}和OLM\upcite{Rlmolm}解决On\-the\-fly路由算法的问题，
减少虚通道数量并保证网络性能。
Won等人在2015年HPCA会议上提出了避免
Dragonfly远端拥塞的路由算法\upcite{ofc}，
通过添加历史窗口观察正在链路上传输的报文以判断应该走最短路径还是非最短路径。

拥塞控制机制对于高性能互连网络，重要性不低于路由算法。
Kim等人在\upcite{cbcm}中指出，路由算法是解决网络中链路拥塞，
拥塞控制机制则是解决终端拥塞。
路由算法和拥塞控制机制两者密切相关，
相互作用，相互影响，结果都是作用在网络性能上。
传统的Explicit Congestion Notification（ECN）
拥塞控制机制已经广泛使用在高性能互连网络，
如在InfiniBand Architecture\upcite{ecn1}\upcite{ecn2}\upcite{ecn3}。
但是，ECN机制对参数敏感，对拥塞情况反馈时间长，
不能准确及时的对拥塞采取措施。
尤其，高性能互连网络是一个无损网络，不允许网络中有丢包现象，
如果网络中发生拥塞，对网络性能影响巨大。
因此，Kim等人提出的CBCM 策略\upcite{cbcm}针对反馈时间长，
参数敏感等缺点做了有效的改进，
通过引入竞争度的概念，及时监测拥塞并作出反馈。
Jiang等人则提出了预约的方式主动避免终端拥塞\upcite{srp}\upcite{crp}\upcite{lhrp}。
除此之外，还有研究者把解决拥塞的关键放在降低Head-of-Line Blocking（HoLB）
出现概率，如FlexVC\upcite{flexvc}以及Y$\'{e}$benes等人的
相关研究 \upcite{Y2017Providing}\upcite{Y2017An}\upcite{Y2018Head}，
有效的利用虚拟通道减少HoLB的发生。

E级计算时代即将到来标志着高性能互连网络也将要进入一个新时代，
但是，这同时也给高性能互连网络现有技术带来了巨大挑战。
目前的高性能互连网络新型拓扑结构无法同时满足E级计算规模、
灵活性、器件约束、物理封装、网络性能、应用负载等需求，
已有的路由算法和拥塞控制机制也随着拓扑规模、
网络直径、 应用负载等变化面临新的挑战。

\section{论文的主要工作}
本文面向高性能互连网络，
针对当前新型高性能互连网络在物理器件约束下如何满足E级计算系统网络规模、
灵活性以及应用负载的需求，路由算法如何解决缓存资源利用率低等问题，
分别对灵活的、满足应用负载的新型高性能互连网络以及
高效利用缓存资源的路由算法等具有挑战性的问题进行深入的研究，
论文的主要工作总结如下：

（1）针对在E级计算的挑战下，
因路由器端口数约束使当前高性能互连网络的灵活性及网络性能方面的不足，
提出一种灵活端口数的高性能互连网络新型拓扑结构Galaxyfly。
Galaxyfly利用代数图论有限域的方法构造而成。
在保持低直径的情况下，Galaxyfly可以达到网络规模和二分带宽的灵活权衡。
其降低了对高阶路由器端口数的要求，
可以使用较少的端口数去构建E级计算系统的网络规模。
针对Galaxyfly结构，不仅分析了可构造的配置并且评估了最短路径数量。
利用其代数图论的性质，设计了拥塞敏感的路由算法。
与其他新型高性能互连网络拓扑结构分别从性能、
成本和能耗三方面进行了实际物理布局的模拟和分析比较。
结果表明Galaxyfly相比其他结构，在不同的路由算法以及典型的通信模式下，
能够展现更优的性能，是一个适合构建E级计算系统的新型高性能互连网络拓扑结构。

（2）针对在E级计算的挑战下，
目前高性能互连网络的网络性能、可维护性以及物理封装方面的不足，
提出一种适合使用多芯光纤的高性能互连网络新型拓扑结构Bundlefly。
Bundlefly是一个低直径、可灵活扩展并且适合采用多芯光纤连接机柜之间的拓扑结构。
随着集成光模块板的发展，一根多芯光纤可以替代一捆传统的单芯光纤，
不仅可以降低光纤的使用成本还可以提高光纤的可维护性。
虽然Bundlefly的网络直径只有3，
但是其不仅能够充分利用多芯光纤来提高机柜间的通信带宽
还能降低高阶路由器的端口数的要求来支持E级系统的可扩展性。
通过分析和模拟，比较了Bundlefly和其他新型高性能互连网络拓扑结构，
Bundlefly能够完成更好的性能。

（3）针对目前高性能互连网络自适应路由算法对虚拟通道数量要求高以及
缓存资源利用均衡的不足，提出了一种标签路由算法Label-based Routing(LBR)。
LBR通过协同设计路由器微体系结构里的输入缓冲区模块和路由计算模块，
将路由计算引入缓冲区模块，
根据网络状态对路由报文做标记。
LBR不仅降低了死锁避免政策对虚拟通道的需求，
还均衡使用缓存资源并有效实现完全自适应路由。
通过模拟在Dragonfly结构上评估了LBR的性能并与其他拓扑结构进行了对比。
实验表明，在大部分通信模式下，LBR优于别的路由算法近10\%-35\%。

\section{论文的组织结构}

本文紧紧围绕高性能互连网络新型拓扑结构和路由算法进行优化设计，
本文共分为六章。
论文组织框图如图\ref{orgth}及各章节主要内容介绍如下：


\begin{figure}[htp]
\centering
\includegraphics[width=.88\textwidth,height=.45\textwidth]{Visio-org_th.eps}
\caption{论文组织框图}
\label{orgth}
\end{figure}

第一章绪论，首先对高性能计算以及高性能计算机进行介绍，
并论述了高性能互连网络的研究意义以及高性能互连网络设计需要考虑的因素，
然后对高性能互连网络的研究现状以及不足进行分析，
最后简述了本文的主要研究内容和组织结构。

第二章介绍了本文的研究技术背景，包括物理器件对高性能互连网络设计的影响，
高性能互连网络新型拓扑结构，
高性能互连网络路由算法及高性能互连网络拥塞控制机制。

第三章提出了一种灵活端口数的高性能互连网络新型拓扑结构Galaxyfly。
首先详细介绍了Galaxyfly拓扑结构的构造方法，
然后对Galaxyfly结构的灵活性、可构造配置、
最短路径数以及容错性进行了详细的理论分析，
之后描述了Galaxyfly结构的路由算法。
最后通过模拟仿真对Galaxyfly的网络性能进行验证。

第四章提出了一种适合多芯光纤的高性能互连网络新型拓扑结构Bundlefly。
首先详细介绍了Bundlefly拓扑结构的构造方法，
然后对Bundlefly结构的网络直径、可扩展性、二分带宽、最短路径数、平均最短路径
以及容错性进行了详细的理论分析，
之后描述了Bundlefly结构的路由算法。
最后通过模拟仿真对Bundlefly的网络性能进行验证。

第五章提出了一种基于标签的高性能互连网络路由算法Label-based Routing。
首先详细介绍了Label-based Routing算法的整体架构，
然后论述Label-based Routing算法的缓冲区、
路由算法以及虚拟通道分配机制，
最后通过模拟仿真对Label-based Routing的性能进行验证。

第六章结论与展望，对全文的研究内容和主要工作进行总结，并对后续的工作进行展望。
